{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89df8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b3a2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.metrics import mean_absolute_error \n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pytvfemd import tvfemd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200a3012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = [], []\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104b7471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input_data(scaled, n_in, n_out=144, n_vars=4, train_proportion=0.8):    \n",
    "    level = scaled[:, 0]\n",
    "    q = scaled[:, 1]\n",
    "    level_tvfemd = tvfemd(level)\n",
    "    q_tvfemd = tvfemd(q)    \n",
    "    level_q_tvfemd = np.hstack((level_tvfemd, q_tvfemd))\n",
    "    input_reframed = series_to_supervised(level_q_tvfemd, n_in, n_out)\n",
    "    input_contain_vars = []\n",
    "    for i in range(1, n_in+1):\n",
    "        input_contain_vars += [('var%d(t-%d)' % (j, i)) for j in range(1,n_vars+1)]  \n",
    "    input_data = input_reframed [ input_contain_vars + ['var1(t)'] + [('var1(t+%d)' % (j)) for j in range(1,n_out)]]\n",
    "    input_col_names = ['X0', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X12', \n",
    "                       'X13', 'X14', 'X15', 'X16', 'X17', 'X18', 'X19', 'X20', 'X21', 'X22', 'X23']\n",
    "    input_contain_vars = []\n",
    "    for i in range(n_vars):\n",
    "        input_contain_vars += [('%s(t-%d)' % (input_col_names[i], j)) for j in range(1,n_in+1)]  \n",
    "    input_data.columns = input_contain_vars +  ['X0(t)'] + [('X0(t+%d)' % (j)) for j in range(1,n_out)]\n",
    "    input_values = input_data.values\n",
    "    input_n_train = round(input_data.shape[0]*train_proportion)\n",
    "    input_train = input_values[:input_n_train, :]\n",
    "    input_test = input_values[input_n_train:, :]\n",
    "    input_train_X, input_train_y = input_train[:, :n_in*n_vars], input_train[:, n_in*n_vars:]\n",
    "    input_test_X, input_test_y = input_test[:, :n_in*n_vars], input_test[:, n_in*n_vars:]\n",
    "    input_train_X = input_train_X.reshape((input_train_X.shape[0], n_in, n_vars))\n",
    "    input_test_X = input_test_X.reshape((input_test_X.shape[0], n_in, n_vars))\n",
    "    \n",
    "    return input_train_X, input_test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac17c95-f0e1-4ce9-915b-c36f28f6a2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_output_data(scaled, n_in, n_out=144, n_vars=4, train_proportion=0.8):\n",
    "    reframed = series_to_supervised(scaled, n_in, n_out)\n",
    "    contain_vars = []\n",
    "    for i in range(1, n_in+1):\n",
    "        contain_vars += [('var%d(t-%d)' % (j, i)) for j in range(1,n_vars+1)]  \n",
    "    data = reframed [ contain_vars + ['var1(t)'] + [('var1(t+%d)' % (j)) for j in range(1,n_out)]]\n",
    "    col_names = ['Y', 'X1', 'X2', 'X3']\n",
    "    contain_vars = []\n",
    "    for i in range(n_vars):\n",
    "        contain_vars += [('%s(t-%d)' % (col_names[i], j)) for j in range(1,n_in+1)]  \n",
    "    data.columns = contain_vars +  ['Y(t)'] + [('Y(t+%d)' % (j)) for j in range(1,n_out)]\n",
    "    values = data.values\n",
    "    n_train = round(data.shape[0]*train_proportion)\n",
    "    train = values[:n_train, :]\n",
    "    test = values[n_train:, :]\n",
    "    train_X, train_y = train[:, :n_in*n_vars], train[:, n_in*n_vars:]\n",
    "    test_X, test_y = test[:, :n_in*n_vars], test[:, n_in*n_vars:]\n",
    "    train_X = train_X.reshape((train_X.shape[0], n_in, n_vars))\n",
    "    test_X = test_X.reshape((test_X.shape[0], n_in, n_vars))\n",
    "    \n",
    "    return train_y, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338bc87f-fb93-43a6-b3f5-2847af673757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filepath):\n",
    "    dataset = read_csv(filepath, encoding='utf-8', index_col=0)\n",
    "    values = dataset.values\n",
    "    values = values.astype('float32')\n",
    "       \n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled = scaler.fit_transform(values)\n",
    "    return scaler, scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cce368-6af0-4a60-be06-2d06e642daa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset = read_csv(r'D:\\Liuzhan\\Data.csv', encoding='utf-8', index_col=0)\n",
    "Values = Dataset.values\n",
    "Values = Values.astype('float32')       \n",
    "Scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "Scaled = Scaler.fit_transform(Values)\n",
    "\n",
    "Level = Scaled[:, 0]\n",
    "Q = Scaled[:, 1]\n",
    "Level_tvfemd = tvfemd(Level)\n",
    "Q_tvfemd = tvfemd(Q)    \n",
    "Level_Q_tvfemd = np.hstack((Level_tvfemd, Q_tvfemd))\n",
    "np.savetxt(r'D:\\Liuzhan\\TVFEMD Result.csv', Level_Q_tvfemd, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377037ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r'D:\\Liuzhan\\Data.csv'\n",
    "n_in = 28\n",
    "n_out = 4\n",
    "n_vars_No_D = 2\n",
    "n_vars_D = 24\n",
    "inv_yhat_list = []\n",
    "inv_y_list = []\n",
    "validation = 0.8\n",
    "scaler, scaled = read_data(filepath)\n",
    "train_X, test_X = prepare_input_data(scaled, n_in, n_out, n_vars = n_vars_D, train_proportion=validation) \n",
    "train_Y, test_Y = prepare_output_data(scaled, n_in, n_out, n_vars = n_vars_No_D, train_proportion=validation)\n",
    "\n",
    "print('type(train_X) = ', type(train_X))\n",
    "print('train_X.shape = ', train_X.shape)\n",
    "print('type(train_Y) = ', type(train_Y))\n",
    "print('train_Y.shape = ', train_Y.shape)\n",
    "print('type(test_X) = ', type(test_X))\n",
    "print('test_X.shape = ', test_X.shape)\n",
    "print('type(test_Y) = ', type(test_Y))\n",
    "print('test_Y.shape = ', test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f2711d-42ce-475f-a2dc-f227b49820aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_2D = train_X.reshape((train_X.shape[0], n_in*n_vars_D))\n",
    "test_X_2D = test_X.reshape((test_X.shape[0], n_in*n_vars_D))\n",
    "np.savetxt(r'D:\\Liuzhan\\train_X.csv', train_X_2D, delimiter=\",\")\n",
    "np.savetxt(r'D:\\Liuzhan\\train_Y.csv', train_Y, delimiter=\",\")  \n",
    "np.savetxt(r'D:\\Liuzhan\\test_X.csv', test_X_2D, delimiter=\",\") \n",
    "np.savetxt(r'D:\\Liuzhan\\test_Y.csv', test_Y, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a016d4-6aad-46f3-9e1a-27a82ca3a5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from kan import KAN\n",
    "from torch.nn import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11402ea9-e049-4825-aa64-30218b4ff2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = torch.from_numpy(train_X).type(torch.Tensor)\n",
    "test_X = torch.from_numpy(test_X).type(torch.Tensor)\n",
    "train_Y = torch.from_numpy(train_Y).type(torch.Tensor)\n",
    "test_Y = torch.from_numpy(test_Y).type(torch.Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9256017d-923b-4a34-a8fb-594d8af8f833",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dafd6e-8e32-4bbd-b617-73676444dd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TVFEMD_EA_CNN_LSTM_KAN(nn.Module):\n",
    "    hidden_cell_num = []\n",
    "    def __init__(self, In_Channels, Out_Channels, Input_Size, Output_Size, Batch_Size, dropout, S, cell_num):\n",
    "        super().__init__()\n",
    "        self.output_size = Output_Size\n",
    "        self.num_directions = 1\n",
    "        self.batch_size = Batch_Size\n",
    "        self.hidden_cell_num = cell_num\n",
    "\n",
    "        self.mk=nn.Linear(Input_Size,S,bias=False)\n",
    "        self.softmax=nn.Softmax(dim=2)  \n",
    "        self.mv=nn.Linear(S,Input_Size,bias=False)        \n",
    "        self.init_weights()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=In_Channels, out_channels=Out_Channels, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=Out_Channels,  \n",
    "                      out_channels=Out_Channels, \n",
    "                      kernel_size=3),   \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=1)            \n",
    "        )\n",
    "        \n",
    "        self.lstm0 = nn.LSTMCell(Out_Channels, hidden_size=self.hidden_cell_num[0])\n",
    "        self.dropout0 = nn.Dropout(p=dropout)\n",
    "        self.lstm1 = nn.LSTMCell(input_size=self.hidden_cell_num[0], hidden_size=self.hidden_cell_num[1])  \n",
    "        self.dropout1 = nn.Dropout(p=dropout)\n",
    "        self.lstm2 = nn.LSTMCell(input_size=self.hidden_cell_num[1], hidden_size=self.hidden_cell_num[2])  \n",
    "        self.dropout2 = nn.Dropout(p=dropout)\n",
    "        self.lstm3 = nn.LSTMCell(input_size=self.hidden_cell_num[2], hidden_size=self.hidden_cell_num[3]) \n",
    "        self.dropout3 = nn.Dropout(p=dropout)\n",
    "        self.lstm4 = nn.LSTMCell(input_size=self.hidden_cell_num[3], hidden_size=self.hidden_cell_num[4])  \n",
    "        self.dropout4 = nn.Dropout(p=dropout)\n",
    "        \n",
    "        self.kan=KAN(width=[self.hidden_cell_num[4], 1, 1, 3, Output_Size], grid=20, k=3).to(device)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        batch_size, seq_len = input_seq.shape[0], input_seq.shape[1]\n",
    "        # batch_size, hidden_size\n",
    "        h_l0 = torch.zeros(batch_size, self.hidden_cell_num[0]).to(device)\n",
    "        c_l0 = torch.zeros(batch_size, self.hidden_cell_num[0]).to(device)\n",
    "        h_l1 = torch.zeros(batch_size, self.hidden_cell_num[1]).to(device)\n",
    "        c_l1 = torch.zeros(batch_size, self.hidden_cell_num[1]).to(device)\n",
    "        h_l2 = torch.zeros(batch_size, self.hidden_cell_num[2]).to(device)\n",
    "        c_l2 = torch.zeros(batch_size, self.hidden_cell_num[2]).to(device)\n",
    "        h_l3 = torch.zeros(batch_size, self.hidden_cell_num[3]).to(device)\n",
    "        c_l3 = torch.zeros(batch_size, self.hidden_cell_num[3]).to(device)\n",
    "        h_l4 = torch.zeros(batch_size, self.hidden_cell_num[4]).to(device)\n",
    "        c_l4 = torch.zeros(batch_size, self.hidden_cell_num[4]).to(device)\n",
    "\n",
    "        attn = self.mk(input_seq) \n",
    "        attn = self.softmax(attn) \n",
    "        attn = attn/torch.sum(attn,dim=1,keepdim=True) \n",
    "        out = self.mv(attn) \n",
    "        x = input_seq + out\n",
    "        \n",
    "        conv = x.permute(0, 2, 1)\n",
    "        conv = self.conv(conv)\n",
    "        conv = conv.permute(0, 2, 1)\n",
    "        conv = self.relu(conv)\n",
    "        \n",
    "        output = []\n",
    "        for t in range(conv.shape[1]):\n",
    "            h_l0, c_l0 = self.lstm0(conv[:, t, :], (h_l0, c_l0))\n",
    "            h_l0, c_l0 = self.dropout0(h_l0), self.dropout0(c_l0)\n",
    "            h_l1, c_l1 = self.lstm1(h_l0, (h_l1, c_l1))\n",
    "            h_l1, c_l1 = self.dropout1(h_l1), self.dropout1(c_l1)            \n",
    "            h_l2, c_l2 = self.lstm2(h_l1, (h_l2, c_l2))\n",
    "            h_l2, c_l2 = self.dropout2(h_l2), self.dropout2(c_l2)\n",
    "            h_l3, c_l3 = self.lstm3(h_l2, (h_l3, c_l3))\n",
    "            h_l3, c_l3 = self.dropout3(h_l3), self.dropout3(c_l3)\n",
    "            h_l4, c_l4 = self.lstm3(h_l3, (h_l4, c_l4))\n",
    "            h_l4, c_l4 = self.dropout3(h_l4), self.dropout3(c_l4)\n",
    "            output.append(h_l4)\n",
    "\n",
    "        pred = self.kan(output[-1])\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init.constant_(m.weight, 1)\n",
    "                init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.normal_(m.weight, std=0.001)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80734853-5a5d-41b0-8aca-e2e272ab86af",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_num = [700, 700, 600, 600, 600]  \n",
    "model = TVFEMD_EA_CNN_LSTM_KAN(In_Channels=n_vars_D, Out_Channels=64, Input_Size = n_vars_D, Output_Size = n_out, Batch_Size=1, dropout=0.3, S=64, cell_num = cell_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42a1ec1-6729-42d2-afa5-987df0592c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "train_X = train_X.to(device)\n",
    "test_X = test_X.to(device)\n",
    "train_Y = train_Y.to(device)\n",
    "test_Y = test_Y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160a22f3-78b4-4e31-8cc8-943ad81f2e61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "\n",
    "n_epochs = 10000\n",
    "learning_rate = 0.001\n",
    "epsilon = 0.0001\n",
    "criterion = torch.nn.MSELoss(reduction='mean') # mean value of loss on all samples\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate, eps=epsilon)\n",
    "\n",
    "loss_hist = np.zeros(n_epochs)\n",
    "val_loss_hist = np.zeros(n_epochs)\n",
    "start_time = time.time()\n",
    "print(\"Start training timeï¼š \", datetime.datetime.now())\n",
    "\n",
    "for t in range(n_epochs):\n",
    "    y_train_pred = model(train_X)\n",
    "    y_test_pred = model(test_X)\n",
    "    loss = criterion(y_train_pred, train_Y)\n",
    "    val_loss = criterion(y_test_pred, test_Y)    \n",
    "    loss_hist[t] = loss.item()\n",
    "    val_loss_hist[t] = val_loss.item()\n",
    "    print(\"Epoch \", t, \"/\", n_epochs, \" train MSE: \", loss.item(), \" ;  test MSE: \", val_loss.item())\n",
    "    optimiser.zero_grad() \n",
    "    loss.backward()       \n",
    "    optimiser.step()      \n",
    "    torch.cuda.empty_cache() \n",
    "    \n",
    "training_time = time.time() - start_time\n",
    "print(\"Training time: {}\".format(training_time))\n",
    "\n",
    "csv_path = r'D:\\Liuzhan\\training process.csv'\n",
    "np.savetxt(csv_path, np.column_stack((loss_hist, val_loss_hist)), delimiter=\",\", header=\"Training loss,Test loss\", comments='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9f7387-f083-4e26-a8a8-7cd60b1bf131",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.figure(figsize=(5,4),dpi=400)  \n",
    "\n",
    "plt.plot(loss_hist, color='blue', label='train')\n",
    "plt.plot(val_loss_hist, color='red', label='test')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend([\"Train loss\", \"Test loss\"], ncol=1, prop={\"size\": 10})  # \"family\": \"Times New Roman\", \n",
    "plt.title('MSE - Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf95ee9-1cdc-4746-9141-0940c89c6736",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "print('type(test_X) = ', type(test_X))\n",
    "print('test_X.shape = ', test_X.shape)\n",
    "yhat = model(test_X)\n",
    "print('type(yhat) = ', type(yhat))\n",
    "print('yhat.shape = ', yhat.shape)\n",
    "\n",
    "inv_yhat_list = []\n",
    "inv_y_list = []\n",
    "inv_y = [] \n",
    "inv_yhat = []\n",
    "\n",
    "scale_new = MinMaxScaler()\n",
    "scale_new.min_, scale_new.scale_ = scaler.min_[0], scaler.scale_[0] \n",
    "inv_yhat = scale_new.inverse_transform(yhat.detach().numpy()) \n",
    "inv_y = scale_new.inverse_transform(test_Y.detach().numpy()) \n",
    "inv_yhat_list.append(inv_yhat)\n",
    "inv_y_list.append(inv_y)\n",
    "\n",
    "print('type(inv_yhat) = ', type(inv_yhat))\n",
    "print('inv_yhat.shape = ', inv_yhat.shape)\n",
    "print('type(inv_y) = ', type(inv_y))\n",
    "print('inv_y.shape = ', inv_y.shape)\n",
    "csv_path = r'D:\\Liuzhan\\Actual.csv'\n",
    "np.savetxt(csv_path, inv_y, delimiter=\",\", header=\"Actual\", comments='')\n",
    "csv_path = r'D:\\Liuzhan\\Predicted.csv'\n",
    "np.savetxt(csv_path, inv_yhat, delimiter=\",\", header=\"Predicted\", comments='')\n",
    "\n",
    "MSE_level = []\n",
    "RMSE_level = []\n",
    "MAPE_level = []\n",
    "MAE_level = []\n",
    "   \n",
    "for j in range(inv_yhat.shape[0]):\n",
    "    mse_level = mean_squared_error(inv_y[j], inv_yhat[j])\n",
    "    rmse_level = sqrt(mean_squared_error(inv_y[j], inv_yhat[j]))\n",
    "    mape_level = mean_absolute_percentage_error(inv_y[j], inv_yhat[j])\n",
    "    mae_level = mean_absolute_error(inv_y[j], inv_yhat[j])\n",
    "        \n",
    "    MSE_level.append(mse_level)\n",
    "    RMSE_level.append(rmse_level)\n",
    "    MAPE_level.append(mape_level)\n",
    "    MAE_level.append(mae_level)\n",
    "    \n",
    "print('')\n",
    "print('MSE = ', mean(MSE_level)) \n",
    "print('RMSE = ', mean(RMSE_level))\n",
    "print('MAPE = ', mean(MAPE_level))\n",
    "print('MAE = ', mean(MAE_level))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcf2937-7167-436f-86d5-bcd89d479adb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
